"""
Integraci√≥n de LangGraph con ROS 2 para an√°lisis de im√°genes y toma de decisiones.
Este m√≥dulo adapta las funciones del UAV con im√°genes de ROS 2.
"""

import os
import cv2
import base64
import json
import time
import re
from typing import Dict, List, Optional, Tuple, Any
import numpy as np
from cv_bridge import CvBridge

# Detecci√≥n geom√©trica deshabilitada - usar solo GPT
GEOMETRIC_DETECTION_AVAILABLE = False

# Importar desde MultiAgent si est√° disponible
try:
    import sys
    multiagent_path = os.path.join(os.path.dirname(__file__), "..", "..", "..", "..", "MultiAgent")
    if os.path.exists(multiagent_path):
        sys.path.insert(0, multiagent_path)
    from langchain_openai import ChatOpenAI
    from dotenv import load_dotenv
    
    # Cargar .env desde MultiAgent
    multiagent_env_path = os.path.join(multiagent_path, ".env")
    load_dotenv(multiagent_env_path)
    
    llm = ChatOpenAI(model="gpt-5-mini", api_key=os.getenv("OPENAI_API_KEY"))
    llm5 = ChatOpenAI(model="gpt-5", api_key=os.getenv("OPENAI_API_KEY"))
except Exception as e:
    print(f"‚ö†Ô∏è Advertencia: No se pudo cargar LangChain. Algunas funciones no estar√°n disponibles: {e}")
    llm = None


class UAVState(dict):
    """
    Estado del agente UAV adaptado para ROS 2.
    Similar al State original pero adaptado para trabajar en tiempo real.
    """
    pass


def preprocess_image_for_detection(frame: np.ndarray) -> np.ndarray:
    """
    Preprocesa una imagen para mejorar la detecci√≥n:
    - Mejora de contraste (CLAHE)
    - Reducci√≥n de ruido (filtro bilateral)
    - Normalizaci√≥n de brillo
    
    Args:
        frame: Frame de OpenCV (numpy array)
        
    Returns:
        Frame preprocesado
    """
    try:
        # Convertir a RGB si est√° en BGR
        if len(frame.shape) == 3 and frame.shape[2] == 3:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        else:
            frame_rgb = frame.copy()
        
        # Aplicar CLAHE (Contrast Limited Adaptive Histogram Equalization) para mejorar contraste
        if len(frame_rgb.shape) == 3:
            lab = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            l = clahe.apply(l)
            frame_rgb = cv2.merge([l, a, b])
            frame_rgb = cv2.cvtColor(frame_rgb, cv2.COLOR_LAB2RGB)
        
        # Reducci√≥n de ruido con filtro bilateral (preserva bordes)
        frame_rgb = cv2.bilateralFilter(frame_rgb, 5, 50, 50)
        
        return frame_rgb
    except Exception as e:
        print(f"‚ö†Ô∏è Error en preprocesamiento de imagen, usando original: {e}")
        return frame.copy() if len(frame.shape) == 3 else frame


def encode_frame_to_base64(frame: np.ndarray, frame_number: int = 0, 
                           use_preprocessing: bool = True, 
                           use_png: bool = False,
                           save_to_file: bool = True) -> str:
    """
    Codifica un frame de OpenCV a base64 para enviarlo a GPT.
    Incluye preprocesamiento opcional para mejorar la detecci√≥n.
    
    Args:
        frame: Frame de OpenCV (numpy array)
        frame_number: N√∫mero de frame para logging
        use_preprocessing: Si True, aplica preprocesamiento de imagen
        use_png: Si True, usa PNG (sin p√©rdida), si False usa JPEG (m√°s r√°pido)
        save_to_file: Si True, guarda el base64 en MultiAgent/frame_base64_1.txt
        
    Returns:
        String base64 codificado
    """
    try:
        # Preprocesar imagen si est√° habilitado
        if use_preprocessing:
            frame_processed = preprocess_image_for_detection(frame)
        else:
            if len(frame.shape) == 3 and frame.shape[2] == 3:
                frame_processed = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            else:
                frame_processed = frame
        
        # Codificar a PNG (sin p√©rdida) o JPEG (m√°s r√°pido)
        if use_png:
            _, buffer = cv2.imencode('.png', frame_processed)
        else:
            # JPEG con mayor calidad para mejor detecci√≥n
            _, buffer = cv2.imencode('.jpg', frame_processed, [cv2.IMWRITE_JPEG_QUALITY, 95])
        
        frame_base64 = base64.b64encode(buffer).decode('utf-8')
        
        # Guardar en archivo si est√° habilitado (imagen exacta que se env√≠a a GPT)
        if save_to_file:
            try:
                # Buscar directorio MultiAgent
                current_dir = os.path.dirname(os.path.abspath(__file__))
                multiagent_path = os.path.join(current_dir, "..", "..", "..", "..", "MultiAgent")
                
                if os.path.exists(multiagent_path):
                    base64_filename = os.path.join(multiagent_path, "frame_base64_1.txt")
                    with open(base64_filename, "w", encoding="utf-8") as f:
                        f.write(frame_base64)
                    print(f"üíæ Base64 guardado en: {base64_filename}")
                else:
                    # Intentar guardar en el directorio actual si no se encuentra MultiAgent
                    base64_filename = os.path.join(current_dir, "frame_base64_1.txt")
                    with open(base64_filename, "w", encoding="utf-8") as f:
                        f.write(frame_base64)
                    print(f"üíæ Base64 guardado en: {base64_filename}")
            except Exception as e:
                print(f"‚ö†Ô∏è Error guardando base64 en archivo: {e}")
        
        return frame_base64
    except Exception as e:
        print(f"‚ùå Error codificando frame {frame_number}: {e}")
        return ""


def validate_coordinates(coords: Dict, width: int, height: int, 
                        min_distance: float = 5.0) -> bool:
    """
    Valida que las coordenadas est√©n dentro del rango v√°lido de la imagen.
    
    Args:
        coords: Diccionario con 'x' e 'y'
        width: Ancho de la imagen
        height: Alto de la imagen
        min_distance: Distancia m√≠nima desde los bordes (p√≠xeles)
        
    Returns:
        True si las coordenadas son v√°lidas
    """
    x = coords.get('x', -1)
    y = coords.get('y', -1)
    
    return (min_distance <= x <= width - min_distance and 
            min_distance <= y <= height - min_distance)


def filter_duplicate_detections(detections: List[Dict], 
                                 min_distance_pixels: float = 10.0) -> List[Dict]:
    """
    Filtra detecciones duplicadas que est√°n muy cerca entre s√≠.
    Mantiene la detecci√≥n con mayor confianza o la primera encontrada.
    
    Args:
        detections: Lista de detecciones con coordenadas 'x' e 'y'
        min_distance_pixels: Distancia m√≠nima en p√≠xeles para considerar duplicados
        
    Returns:
        Lista filtrada de detecciones
    """
    if not detections:
        return []
    
    filtered = []
    for detection in detections:
        coords = detection.get('coordenadas', {})
        x = coords.get('x', 0)
        y = coords.get('y', 0)
        
        is_duplicate = False
        for existing in filtered:
            existing_coords = existing.get('coordenadas', {})
            ex = existing_coords.get('x', 0)
            ey = existing_coords.get('y', 0)
            
            distance = np.sqrt((x - ex)**2 + (y - ey)**2)
            if distance < min_distance_pixels:
                is_duplicate = True
                break
        
        if not is_duplicate:
            filtered.append(detection)
    
    return filtered


def extract_json_from_response(response_text: str) -> Optional[Dict]:
    """
    Extrae JSON de una respuesta de GPT de forma m√°s robusta.
    Intenta m√∫ltiples m√©todos para encontrar el JSON v√°lido.
    
    Args:
        response_text: Texto de respuesta de GPT
        
    Returns:
        Diccionario con el JSON parseado, o None si falla
    """
    if not response_text:
        return None
    
    # M√©todo 1: Buscar entre llaves simples
    json_start = response_text.find('{')
    json_end = response_text.rfind('}') + 1
    
    if json_start != -1 and json_end > json_start:
        try:
            json_str = response_text[json_start:json_end]
            result = json.loads(json_str)
            return result
        except json.JSONDecodeError:
            pass
    
    # M√©todo 2: Buscar bloques de c√≥digo JSON
    json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
    matches = re.findall(json_pattern, response_text, re.DOTALL)
    if matches:
        try:
            result = json.loads(matches[0])
            return result
        except json.JSONDecodeError:
            pass
    
    # M√©todo 3: Buscar cualquier estructura JSON v√°lida
    json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
    matches = re.findall(json_pattern, response_text, re.DOTALL)
    for match in matches:
        try:
            result = json.loads(match)
            if isinstance(result, dict):
                return result
        except json.JSONDecodeError:
            continue
    
    return None


def classify_victims_with_gpt(frame_base64: str, candidates: List[Dict], 
                               width: int, height: int) -> Dict:
    """
    Clasifica candidatos a v√≠ctimas usando GPT para validar y clasificar color/estado.
    
    Args:
        frame_base64: Frame codificado en base64
        candidates: Lista de candidatos detectados geom√©tricamente
        width: Ancho de la imagen
        height: Alto de la imagen
    
    Returns:
        Diccionario con v√≠ctimas clasificadas
    """
    if llm is None:
        return {"victimas_identificadas": [], "total_victimas": 0}
    
    # Crear resumen de candidatos
    candidates_text = ""
    for i, candidate in enumerate(candidates, 1):
        candidates_text += f"Candidato {i}: Posici√≥n ({candidate['x']}, {candidate['y']}), "
        candidates_text += f"Radio {candidate.get('radius', 'N/A')}px, "
        candidates_text += f"Color sugerido: {candidate.get('color_hint', 'unknown')}\n"
    
    prompt = f"""
    Eres un sistema de clasificaci√≥n de v√≠ctimas para rescate en desastres.
    
    Se han detectado geom√©tricamente {len(candidates)} candidatos a v√≠ctimas (c√≠rculos de colores).
    Tu tarea es VALIDAR cada candidato y CLASIFICAR su color/estado.
    
    INFORMACI√ìN DE LA IMAGEN:
    - Resoluci√≥n: {width} x {height} p√≠xeles
    
    CANDIDATOS DETECTADOS:
    {candidates_text}
    
    INSTRUCCIONES:
    1. Para CADA candidato, verifica si es realmente una v√≠ctima (c√≠rculo de color)
    2. Clasifica el color exacto: ROJO, NARANJA, o VERDE OSCURO
    3. Determina el estado seg√∫n el color:
       - ROJO = cr√≠tico (prioridad alta)
       - NARANJA = herido (prioridad media)
       - VERDE OSCURO = seguro (prioridad baja)
    4. Usa las coordenadas EXACTAS proporcionadas (centro del c√≠rculo detectado)
    5. Si un candidato NO es una v√≠ctima (es un obst√°culo u otro objeto), EXCLUYELO
    
    Responde en formato JSON:
    {{
        "victimas_identificadas": [
            {{
                "id": 1,
                "coordenadas": {{"x": 150, "y": 200}},
                "estado": "cr√≠tico/herido/seguro",
                "color": "rojo/naranja/verde oscuro",
                "prioridad": "alta/media/baja",
                "validado": true
            }}
        ],
        "total_victimas": n√∫mero_total,
        "candidatos_rechazados": n√∫mero_rechazados
    }}
    """
    
    try:
        message = {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{frame_base64}",
                        "detail": "high"
                    }
                }
            ]
        }
        
        response = llm.invoke([message])
        analysis_result = response.content
        
        # Extraer JSON
        json_start = analysis_result.find('{')
        json_end = analysis_result.rfind('}') + 1
        
        if json_start != -1 and json_end != -1:
            json_str = analysis_result[json_start:json_end]
            result = json.loads(json_str)
            return result
        else:
            print("‚ö†Ô∏è No se encontr√≥ JSON v√°lido en la respuesta de clasificaci√≥n")
            return {"victimas_identificadas": [], "total_victimas": 0}
            
    except Exception as e:
        print(f"‚ùå Error en clasificaci√≥n de v√≠ctimas: {e}")
        return {"victimas_identificadas": [], "total_victimas": 0}


def identify_victims_from_image(frame: np.ndarray, frame_base64: str = None, 
                                  current_position: Optional[tuple] = None,
                                  use_preprocessing: bool = True) -> Dict:
    """
    Identifica v√≠ctimas en un frame usando GPT-4o Vision.
    
    Args:
        frame: Frame de OpenCV
        frame_base64: Frame codificado en base64
        current_position: Posici√≥n actual del UAV (x, y, z) en metros
        
    Returns:
        Diccionario con v√≠ctimas identificadas
    """
    if llm is None:
        return {"victimas_identificadas": [], "total_victimas": 0}
    
    height, width = frame.shape[:2]
    
    # Recodificar frame con preprocesamiento para mejor detecci√≥n
    # Guardar solo la primera vez (para v√≠ctimas) para evitar duplicados
    if use_preprocessing or frame_base64 is None:
        frame_base64 = encode_frame_to_base64(frame, use_preprocessing=True, use_png=False, save_to_file=True)
    
    prompt = f"""
    Eres un sistema de an√°lisis de video a√©reo para rescate en desastres. 
    
    ‚ö†Ô∏è CR√çTICO: PRECISI√ìN ES FUNDAMENTAL
    T√≥mate TODO el tiempo necesario para analizar la imagen cuidadosamente. 
    La precisi√≥n en las coordenadas es M√ÅS IMPORTANTE que la velocidad del an√°lisis.
    
    INFORMACI√ìN DE LA IMAGEN:
    - Resoluci√≥n: {width} x {height} p√≠xeles
    
    ANALIZA ESTE FRAME ESPEC√çFICAMENTE PARA IDENTIFICAR V√çCTIMAS:
    
    - Las v√≠ctimas aparecen como C√çRCULOS de colores:
      * ROJO (#FF0000, RGB: 255, 0, 0): V√≠ctima cr√≠tica (necesita rescate inmediato)
      * NARANJA (#FF6700, RGB: 255, 103, 0): V√≠ctima herida (necesita atenci√≥n m√©dica)
      * VERDE OSCURO (#006400, RGB: 0, 100, 0): V√≠ctima segura (estable)
    
    - Los obst√°culos son figuras cuadradas de colores:
      * MARR√ìN (#8B4513, RGB: 139, 69, 19) = edificio
      * MAGENTA (#FF00FF, RGB: 255, 0, 255) = escombro
      * AMARILLO (#C8AA3C, RGB: 200, 170, 60) = √°rbol
      * GRIS OSCURO (#404040, RGB: 64, 64, 64) = veh√≠culo
      - NO son v√≠ctimas
    - El fondo es marr√≥n claro con cielo azul
    
    INSTRUCCIONES DETALLADAS:
    1. EXAMINA la imagen con MUCHO cuidado. Identifica CADA c√≠rculo de color.
       - NO confundas los obst√°culos cuadrados con v√≠ctimas
       - NO omitas ninguna v√≠ctima, incluso si est√° parcialmente visible
    2. Para CADA v√≠ctima identificada:
       a. Localiza el CENTRO EXACTO del c√≠rculo con m√°xima precisi√≥n
       b. NO uses el borde del c√≠rculo, sino el PUNTO CENTRAL exacto
       c. Examina cuidadosamente la imagen para determinar las coordenadas pixel (x, y) del centro
       d. Usa el sistema de coordenadas donde (0,0) es la esquina superior izquierda
    3. PRECISI√ìN EN COORDENADAS:
       - Las coordenadas que proporciones se convertir√°n al sistema del mundo
       - Cada p√≠xel de error puede resultar en metros de error en el mundo real
       - El rover depende de estas coordenadas para llegar a las v√≠ctimas
       - T√≥mate el tiempo necesario para medir las coordenadas con precisi√≥n m√°xima
    4. Determina el estado de cada v√≠ctima seg√∫n su color EXACTO:
      - ROJO (#FF0000, RGB: 255, 0, 0) = cr√≠tico
      - NARANJA (#FF6700, RGB: 255, 103, 0) = herido
      - VERDE OSCURO (#006400, RGB: 0, 100, 0) = seguro
    
    Responde en formato JSON:
    {{
        "victimas_identificadas": [
            {{
                "id": 1,
                "coordenadas": {{"x": 150, "y": 200}},
                "estado": "cr√≠tico/herido/atrapado/seguro",
                "color": "rojo/naranja/morado/verde",
                "prioridad": "alta/media/baja"
            }}
        ],
        "total_victimas": n√∫mero_total,
        "resolucion_imagen": {{
            "width": {width},
            "height": {height}
        }}
    }}
    """
    
    try:
        # Enviar imagen y prompt a GPT-4o Vision
        message = {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{frame_base64}",
                        "detail": "high"
                    }
                }
            ]
        }
        
        response = llm.invoke([message])
        analysis_result = response.content
        
        # Extraer JSON de la respuesta (m√©todo mejorado)
        result = extract_json_from_response(analysis_result)
        
        if result is None:
            print("‚ö†Ô∏è No se encontr√≥ JSON v√°lido en la respuesta de v√≠ctimas")
            return {"victimas_identificadas": [], "total_victimas": 0}
        
        # Validar y filtrar coordenadas
        victimas_validas = []
        for victima in result.get("victimas_identificadas", []):
            coords = victima.get("coordenadas", {})
            if validate_coordinates(coords, width, height):
                victimas_validas.append(victima)
            else:
                print(f"‚ö†Ô∏è Coordenadas inv√°lidas descartadas: {coords}")
        
        # Filtrar duplicados
        victimas_validas = filter_duplicate_detections(victimas_validas)
        
        result["victimas_identificadas"] = victimas_validas
        result["total_victimas"] = len(victimas_validas)
        
        return result
            
    except Exception as e:
        print(f"‚ùå Error en identificaci√≥n de v√≠ctimas: {e}")
        import traceback
        traceback.print_exc()
        return {"victimas_identificadas": [], "total_victimas": 0}


def classify_obstacles_with_gpt(frame_base64: str, candidates: List[Dict],
                                 width: int, height: int) -> Dict:
    """
    Clasifica candidatos a obst√°culos usando GPT para validar y clasificar tipo/color.
    
    Args:
        frame_base64: Frame codificado en base64
        candidates: Lista de candidatos detectados geom√©tricamente
        width: Ancho de la imagen
        height: Alto de la imagen
    
    Returns:
        Diccionario con obst√°culos clasificados
    """
    if llm is None:
        return {"obstaculos_identificados": [], "total_obstaculos": 0}
    
    # Crear resumen de candidatos
    candidates_text = ""
    for i, candidate in enumerate(candidates, 1):
        candidates_text += f"Candidato {i}: Posici√≥n ({candidate['x']}, {candidate['y']}), "
        candidates_text += f"Tama√±o {candidate.get('width', 'N/A')}x{candidate.get('height', 'N/A')}px, "
        candidates_text += f"Color sugerido: {candidate.get('color_hint', 'unknown')}\n"
    
    prompt = f"""
    Eres un sistema de clasificaci√≥n de obst√°culos para rescate en desastres.
    
    Se han detectado geom√©tricamente {len(candidates)} candidatos a obst√°culos (cuadrados de colores).
    Tu tarea es VALIDAR cada candidato y CLASIFICAR su tipo seg√∫n el color.
    
    INFORMACI√ìN DE LA IMAGEN:
    - Resoluci√≥n: {width} x {height} p√≠xeles
    
    CANDIDATOS DETECTADOS:
    {candidates_text}
    
    IMPORTANTE: El ROVER es un cuadrado AZUL con borde negro - NO debe ser identificado como obst√°culo.
    
    INSTRUCCIONES:
    1. Para CADA candidato, verifica si es realmente un obst√°culo (cuadrado de color)
    2. EXCLUYE el rover (cuadrado azul con borde negro) si est√° presente
    3. Clasifica el tipo seg√∫n el color:
       - MARR√ìN = edificio/estructura
       - MAGENTA/ROSA = escombro
       - AMARILLO = √°rbol
       - GRIS = veh√≠culo
    4. Usa las coordenadas EXACTAS proporcionadas (centro del cuadrado detectado)
    5. Si un candidato NO es un obst√°culo v√°lido, EXCLUYELO
    
    Responde en formato JSON:
    {{
        "obstaculos_identificados": [
            {{
                "id": 1,
                "coordenadas": {{"x": 150, "y": 200}},
                "tipo": "edificio/escombro/√°rbol/veh√≠culo",
                "color": "marr√≥n/magenta/azul/gris",
                "forma": "cuadrado",
                "validado": true
            }}
        ],
        "total_obstaculos": n√∫mero_total,
        "candidatos_rechazados": n√∫mero_rechazados
    }}
    """
    
    try:
        message = {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{frame_base64}",
                        "detail": "high"
                    }
                }
            ]
        }
        
        response = llm.invoke([message])
        analysis_result = response.content
        
        # Extraer JSON
        json_start = analysis_result.find('{')
        json_end = analysis_result.rfind('}') + 1
        
        if json_start != -1 and json_end != -1:
            json_str = analysis_result[json_start:json_end]
            result = json.loads(json_str)
            return result
        else:
            print("‚ö†Ô∏è No se encontr√≥ JSON v√°lido en la respuesta de clasificaci√≥n de obst√°culos")
            return {"obstaculos_identificados": [], "total_obstaculos": 0}
            
    except Exception as e:
        print(f"‚ùå Error en clasificaci√≥n de obst√°culos: {e}")
        return {"obstaculos_identificados": [], "total_obstaculos": 0}


def identify_obstacles_from_image(frame: np.ndarray, frame_base64: str = None,
                                   use_preprocessing: bool = True) -> Dict:
    """
    Identifica obst√°culos en un frame usando GPT-4o Vision.
    
    Args:
        frame: Frame de OpenCV
        frame_base64: Frame codificado en base64
    
    Returns:
        Diccionario con obst√°culos identificados
    """
    if llm is None:
        return {"obstaculos_identificados": [], "total_obstaculos": 0}
    
    height, width = frame.shape[:2]
    
    # Recodificar frame con preprocesamiento para mejor detecci√≥n
    # No guardar aqu√≠ (ya se guard√≥ en identify_victims_from_image)
    if use_preprocessing or frame_base64 is None:
        frame_base64 = encode_frame_to_base64(frame, use_preprocessing=True, use_png=False, save_to_file=False)
    
    prompt = f"""
    Eres un sistema de an√°lisis de video a√©reo para rescate en desastres.
    
    ‚ö†Ô∏è CR√çTICO: PRECISI√ìN ES FUNDAMENTAL
    T√≥mate TODO el tiempo necesario para analizar la imagen cuidadosamente. 
    La precisi√≥n en las coordenadas es M√ÅS IMPORTANTE que la velocidad del an√°lisis.
    
    INFORMACI√ìN DE LA IMAGEN:
    - Resoluci√≥n: {width} x {height} p√≠xeles
    
    ANALIZA ESTE FRAME ESPEC√çFICAMENTE PARA IDENTIFICAR OBST√ÅCULOS:
    
    - Los obst√°culos son figuras CUADRADAS de color s√≥lido:
      * MARR√ìN (#8B4513, RGB: 139, 69, 19): Edificios/estructuras
      * MAGENTA (#FF00FF, RGB: 255, 0, 255): Escombros
      * AMARILLO (#C8AA3C, RGB: 200, 170, 60): √Årboles
      * GRIS OSCURO (#404040, RGB: 64, 64, 64): Veh√≠culos
    
    - IMPORTANTE: El ROVER es un CUADRADO AZUL (#0000FF, RGB: 0, 0, 255) CON BORDE NEGRO - NO es un obst√°culo
    - Las v√≠ctimas son C√çRCULOS de colores - NO son obst√°culos
    - El fondo es marr√≥n claro con cielo azul
    
    INSTRUCCIONES DETALLADAS:
    1. EXAMINA la imagen con MUCHO cuidado. Identifica CADA figura cuadrada que sea un obst√°culo.
       - NO confundas las v√≠ctimas (c√≠rculos) con obst√°culos (cuadrados)
       - NO incluyas el rover (cuadrado azul con borde negro) como obst√°culo
       - NO omitas ning√∫n obst√°culo, incluso si est√° parcialmente visible
    2. EXCLUYE expl√≠citamente:
       - El ROVER (cuadrado azul con borde negro) - NO es un obst√°culo
       - Las V√çCTIMAS (c√≠rculos de colores) - NO son obst√°culos
    3. Para CADA obst√°culo identificado:
       a. Localiza el CENTRO EXACTO del cuadrado con m√°xima precisi√≥n
       b. NO uses una esquina del cuadrado, sino el PUNTO CENTRAL exacto del cuadrado
       c. Examina cuidadosamente la imagen para determinar las coordenadas pixel (x, y) del centro
       d. Usa el sistema de coordenadas donde (0,0) es la esquina superior izquierda
    4. PRECISI√ìN EN COORDENADAS:
       - Las coordenadas que proporciones se convertir√°n al sistema del mundo
       - Cada p√≠xel de error puede resultar en metros de error en el mundo real
       - El rover depende de estas coordenadas para EVITAR los obst√°culos correctamente
       - T√≥mate el tiempo necesario para medir las coordenadas con precisi√≥n m√°xima
    5. Determina el tipo de obst√°culo seg√∫n su color EXACTO:
      - MARR√ìN (#8B4513, RGB: 139, 69, 19) = edificio
      - MAGENTA (#FF00FF, RGB: 255, 0, 255) = escombro
      - AMARILLO (#C8AA3C, RGB: 200, 170, 60) = √°rbol
      - GRIS OSCURO (#404040, RGB: 64, 64, 64) = veh√≠culo
    
    Responde en formato JSON:
    {{
        "obstaculos_identificados": [
            {{
                "id": 1,
                "coordenadas": {{"x": 150, "y": 200}},
                "tipo": "edificio/escombro/√°rbol/veh√≠culo",
                "forma": "rect√°ngulo/tri√°ngulo/hex√°gono",
                "color": "marr√≥n/magenta/azul/amarillo",
                "tama√±o": "peque√±o/mediano/grande"
            }}
        ],
        "total_obstaculos": n√∫mero_total,
        "resolucion_imagen": {{
            "width": {width},
            "height": {height}
        }}
    }}
    """
    
    try:
        # CORRECCI√ìN: Enviar imagen junto con el prompt (igual que en identify_victims_from_image)
        message = {
            "role": "user",
            "content": [
                {"type": "text", "text": prompt},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{frame_base64}",
                        "detail": "high"
                    }
                }
            ]
        }
        
        response = llm.invoke([message])
        analysis_result = response.content
        
        # Extraer JSON de la respuesta (m√©todo mejorado)
        result = extract_json_from_response(analysis_result)
        
        if result is None:
            print("‚ö†Ô∏è No se encontr√≥ JSON v√°lido en la respuesta de obst√°culos")
            return {"obstaculos_identificados": [], "total_obstaculos": 0}
        
        # Validar y filtrar coordenadas
        obstaculos_validos = []
        for obstaculo in result.get("obstaculos_identificados", []):
            coords = obstaculo.get("coordenadas", {})
            if validate_coordinates(coords, width, height):
                obstaculos_validos.append(obstaculo)
            else:
                print(f"‚ö†Ô∏è Coordenadas inv√°lidas descartadas: {coords}")
        
        # Filtrar duplicados
        obstaculos_validos = filter_duplicate_detections(obstaculos_validos)
        
        result["obstaculos_identificados"] = obstaculos_validos
        result["total_obstaculos"] = len(obstaculos_validos)
        
        return result
            
    except Exception as e:
        print(f"‚ùå Error en identificaci√≥n de obst√°culos: {e}")
        import traceback
        traceback.print_exc()
        return {"obstaculos_identificados": [], "total_obstaculos": 0}


def plan_route(victims: List[Dict], obstacles: List[Dict], 
                start_position: tuple = (0.0, 0.0, 15.0)) -> Dict:
    """
    Planifica una ruta √≥ptima usando GPT.
    
    Args:
        victims: Lista de v√≠ctimas identificadas
        obstacles: Lista de obst√°culos identificados
        start_position: Posici√≥n inicial del UAV (x, y, z)
        
    Returns:
        Diccionario con ruta optimizada
    """
    if llm is None:
        return {"ruta_optimizada": None}
    
    # Formatear datos para el prompt
    victims_data = []
    for i, victim in enumerate(victims, 1):
        coords = victim.get("coordenadas", {})
        x = coords.get("x", 0)
        y = coords.get("y", 0)
        estado = victim.get("estado", "desconocido")
        prioridad = victim.get("prioridad", "media")
        victims_data.append(f"V√≠ctima {i}: Posici√≥n=({x}, {y}), Estado={estado}, Prioridad={prioridad}")
    
    obstacles_data = []
    for i, obstacle in enumerate(obstacles, 1):
        coords = obstacle.get("coordenadas", {})
        x = coords.get("x", 0)
        y = coords.get("y", 0)
        tipo = obstacle.get("tipo", "desconocido")
        obstacles_data.append(f"Obst√°culo {i}: Posici√≥n=({x}, {y}), Tipo={tipo}")
    
    prompt = f"""
    Eres un sistema de planificaci√≥n de rutas para rescate en desastres. 
    
    ‚ö†Ô∏è CR√çTICO: PRECISI√ìN Y COMPLETITUD SON FUNDAMENTALES
    T√≥mate TODO el tiempo necesario para planificar la ruta cuidadosamente. 
    La calidad de la ruta es M√ÅS IMPORTANTE que la velocidad del an√°lisis.
    
    OBJETIVO PRINCIPAL: Planificar UNA SOLA ruta √≥ptima desde la posici√≥n inicial {start_position} que:
    - Pase por TODAS las v√≠ctimas (NO puedes omitir ninguna)
    - Evite TODOS los obst√°culos (manteniendo distancia de seguridad)
    - Minimice la distancia total recorrida
    - Priorice las v√≠ctimas cr√≠ticas primero
    
    V√çCTIMAS A VISITAR (TODAS - OBLIGATORIO VISITAR CADA UNA):
    {chr(10).join(victims_data) if victims_data else "  No hay v√≠ctimas identificadas"}
    
    OBST√ÅCULOS A EVITAR (CR√çTICO - MANTENER DISTANCIA):
    {chr(10).join(obstacles_data) if obstacles_data else "  No hay obst√°culos identificados"}
    
    ‚ö†Ô∏è REGLAS CR√çTICAS DE PLANIFICACI√ìN:
    1. COMPLETITUD OBLIGATORIA:
       - La ruta DEBE pasar por TODAS las v√≠ctimas listadas arriba
       - NO puedes omitir ninguna v√≠ctima, sin importar su prioridad
       - Cada v√≠ctima DEBE estar incluida en la ruta como un punto de paso obligatorio
    
    2. EVITAR OBST√ÅCULOS (CR√çTICO):
       - La ruta DEBE evitar TODOS los obst√°culos listados
       - Mant√©n una distancia de seguridad de al menos 3-5 metros de cada obst√°culo
       - Si un obst√°culo bloquea el camino directo a una v√≠ctima, planifica un desv√≠o alrededor del obst√°culo
       - NO pases a trav√©s de obst√°culos, SIEMPRE pasa alrededor
    
    3. ORDEN DE PRIORIDAD:
       - Prioriza visitar primero las v√≠ctimas con estado "cr√≠tico" o prioridad "alta"
       - Luego visita las v√≠ctimas con estado "herido" o prioridad "media"
       - Finalmente visita las v√≠ctimas con estado "seguro" o prioridad "baja"
    
    4. OPTIMIZACI√ìN:
       - Minimiza la distancia total recorrida
       - Minimiza el n√∫mero de cambios de direcci√≥n innecesarios
       - Pero SIEMPRE prioriza la seguridad (evitar obst√°culos) sobre la distancia
    
    5. PUNTOS DE PASO:
       - Incluye puntos de paso intermedios si son necesarios para evitar obst√°culos
       - Incluye el punto de inicio de la ruta
       - Incluye el punto final despu√©s de visitar todas las v√≠ctimas
    
    ESTRUCTURA DE RESPUESTA:
    - Proporciona una ruta secuencial con todos los puntos de paso
    - Cada punto debe tener coordenadas (x, y) en p√≠xeles
    - Indica el tipo de cada punto: "inicio", "victima", "obstaculo_evitado", "punto_paso"
    - Si es una v√≠ctima, incluye el ID de la v√≠ctima
    
    Responde en formato JSON con la estructura de ruta optimizada completa.
    """
    
    try:
        message = {
            "role": "user",
            "content": prompt
        }
        
        response = llm.invoke([message])
        route_analysis = response.content
        
        # Extraer JSON
        json_start = route_analysis.find('{')
        json_end = route_analysis.rfind('}') + 1
        
        if json_start != -1 and json_end != -1:
            json_str = route_analysis[json_start:json_end]
            route_data = json.loads(json_str)
            return route_data
        else:
            print("‚ö†Ô∏è No se encontr√≥ JSON v√°lido en la respuesta de planificaci√≥n")
            return {"ruta_optimizada": None}
            
    except Exception as e:
        print(f"‚ùå Error en planificaci√≥n de ruta: {e}")
        return {"ruta_optimizada": None}


def convert_image_to_cv2(image_msg) -> Optional[np.ndarray]:
    """
    Convierte un mensaje Image de ROS 2 a un array de OpenCV.
    
    Args:
        image_msg: sensor_msgs.msg.Image
        
    Returns:
        Frame de OpenCV o None si hay error
    """
    try:
        bridge = CvBridge()
        cv_image = bridge.imgmsg_to_cv2(image_msg, desired_encoding='rgb8')
        return cv_image
    except Exception as e:
        print(f"‚ùå Error convirtiendo imagen ROS 2 a OpenCV: {e}")
        return None

